[pak]
name = "vllm-deployment"
version = "1.0.0"
description = "Deploy vLLM for high-performance LLM inference. Covers Docker CPU/GPU deployments and cloud VM provisioning with OpenAI-compatible API endpoints."
authors = ["Stakpak <team@stakpak.dev>"]
license = "MIT"
repository = "https://github.com/stakpak/paks"
path = "vllm-deployment"
keywords = ["vllm", "llm", "inference", "gpu", "ai", "machine-learning", "docker", "openai-api"]
visibility = "public"

[contents]
[[contents.rulebooks]]
path = "rulebooks/vllm-deployment.md"
description = "Deploy vLLM for high-performance LLM inference. Covers Docker CPU/GPU deployments and cloud VM provisioning with OpenAI-compatible API endpoints."
tags = ["vllm", "llm", "inference", "gpu", "ai", "machine-learning", "docker", "openai-api"]
